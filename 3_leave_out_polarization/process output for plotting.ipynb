{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('..')\n",
    "from helpers.funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('../config.json', 'r'))\n",
    "INPUT_DIR = config['INPUT_DIR']\n",
    "OUTPUT_DIR = config['OUTPUT_DIR']\n",
    "TWEET_DIR = config['TWEET_DIR']\n",
    "NUM_CLUSTERS = config['NUM_CLUSTERS']\n",
    "events = open(INPUT_DIR + 'event_names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filter_method = '_noRT' # make sure to include underscore\n",
    "#data_filter_method = '_clustered'\n",
    "#cluster_method = '_relative'\n",
    "cluster_method = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dict = json.load(open(INPUT_DIR + \"event_year.json\",\"r\"))\n",
    "shooter_race = json.load(open(INPUT_DIR + \"shooters_race.json\",\"r\"))\n",
    "polarization_dict = json.load(open(OUTPUT_DIR + \"polarization\"+data_filter_method+ cluster_method+\".json\",\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "between_topic_pol = json.load(open(DATA_DIR + \"between_topic_polarization\"+cluster_method+\".json\",\"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for plotting overall polarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "y_random = []\n",
    "labels=[]\n",
    "ex = ['fort_lauderdale']  # we exclude Fort Lauderdale because we only have data for the first day after the shooting\n",
    "for e, t in time_dict.items():\n",
    "    if e in ex:\n",
    "        continue\n",
    "    label = e.split('_')\n",
    "    new_label = []\n",
    "    for l in label:\n",
    "        new_label.append(l[0].upper() + l[1:])\n",
    "    new_label = ' '.join(new_label)\n",
    "    \n",
    "    x_val = float(2000+t)\n",
    "    y_val = float(polarization_dict[e][0])\n",
    "    y_random_val = float(float(polarization_dict[e][1]))\n",
    "    x.append(x_val)\n",
    "    y.append(y_val)\n",
    "    y_random.append(y_random_val)\n",
    "    labels.append(new_label)\n",
    "    #labels.append(plt.text(x_val, y_val, new_label, fontsize=8))\n",
    "df = pd.DataFrame.from_dict({'year':np.array(x * 2), 'polarization':np.array(y + y_random), 'label':labels * 2, 'is_actual':['actual value']* len(y) + ['value resulting from random party assignment']* len(y)})\n",
    "df.to_csv(DATA_DIR + \"polarization\"+data_filter_method+cluster_method+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for plotting within vs between topic polarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y_between = []\n",
    "y_within = []\n",
    "labels=[]\n",
    "race = []\n",
    "ex = ['fort_lauderdale']\n",
    "for e, t in time_dict.items():\n",
    "    if e in ex:\n",
    "        continue\n",
    "    label = e.split('_')\n",
    "    new_label = []\n",
    "    for l in label:\n",
    "        new_label.append(l[0].upper() + l[1:])\n",
    "    new_label = ' '.join(new_label)\n",
    "    \n",
    "    within_topic_pol = json.load(open(TWEET_DIR +e+\"/\"+e+\"_topic_polarization\"+cluster_method+\".json\",\"r\"))\n",
    "    \n",
    "    # get the topic proportions\n",
    "    topics = pd.read_csv(TWEET_DIR + event + '/' + event + '_kmeans_topics_' + str(NUM_CLUSTERS) + '.csv')\n",
    "    if cluster_method != '':\n",
    "        assigned_indices, topics = get_assigned_indices_relative(topics) if cluster_method == '_relative' else get_assigned_indices_absolute(topics)\n",
    "        \n",
    "    print(e, len(topics))\n",
    "    within_topic_pol_val = (np.array([float(within_topic_pol[str(i)][0]) for i in range(NUM_CLUSTERS)]) * np.bincount(topics['topic_0'])).sum() / len(topics)\n",
    "    \n",
    "    x_val = float(2000+t)\n",
    "    y_between_val = float(between_topic_pol[e][0])\n",
    "    y_within_val = within_topic_pol_val\n",
    "    x.append(x_val)\n",
    "    y_between.append(y_between_val)\n",
    "    y_within.append(y_within_val)\n",
    "    labels.append(new_label)\n",
    "    race.append(shooter_race[e])\n",
    "    #labels.append(plt.text(x_val, y_val, new_label, fontsize=8))\n",
    "df = pd.DataFrame.from_dict({'year':np.array(x * 2), 'polarization':np.array(y_between + y_within), 'label':labels * 2, 'kind':['between-topic']* len(y_between) + ['within-topic']* len(y_within), 'race':race * 2})\n",
    "df.to_csv(DATA_DIR + \"topic_polarization\"+cluster_method+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine polarization over time values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = []\n",
    "for e in events:\n",
    "    vals = np.load(TWEET_DIR + e + '/' + e + '_polarization_over_time_log_noRT.npy')\n",
    "    for i in range(vals.shape[0]):\n",
    "        d = {}\n",
    "        d['event'] = e\n",
    "        d['leaveout'] = vals[i, 0]\n",
    "        d['time'] = vals[i, 3]\n",
    "        d['size'] = vals[i, 2]\n",
    "        d['squared_diff'] = np.abs(vals[i, 1] - .5)\n",
    "        dicts.append(d)\n",
    "df = pd.DataFrame(dicts)\n",
    "df.to_csv(OUTPUT_DIR + 'polarization_over_time_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine user leaveout scores (this is not used in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for e in events:\n",
    "    df = pd.read_csv(TWEET_DIR + e + '/' + e + '_user_leaveout.csv')\n",
    "    df['event'] = e\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.to_csv(TWEET_DIR + 'user_leaveouts.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
